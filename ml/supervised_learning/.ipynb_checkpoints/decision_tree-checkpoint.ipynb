{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/Users/lality/projects/personal/ML_Algo/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml.tools import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionNode:\n",
    "    def __init__(self, feature_i=None, threshold=None, value=None, left_branch=None, right_branch=None):\n",
    "        self.feature_i = feature_i        # Index for feature that is tested\n",
    "        self.threshold = threshold        # Threshold value for a feature\n",
    "        self.value = value                # Value if the node is a leaf in tree\n",
    "        self.left_branch = left_branch    # Left Subtree\n",
    "        self.right_branch = right_branch  # Right Subtree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self, min_samples_split=2, min_impurity=1e-7, max_depth=float(\"inf\"), loss=None):\n",
    "        self.root = None                              # Root node in dec. tree\n",
    "        self.min_samples_split = min_samples_split    # The minimum number of samples required to split an internal node\n",
    "        self.min_impurity = min_impurity              # The minimum impurity in a split  \n",
    "        self.max_depth = max_depth                    # The maximum depth of the tree\n",
    "        self.loss = loss                              # If Gradient Boosting\n",
    "        self._impurity_calculation = None             # Function to determine prediction of y at leaf\n",
    "        self._leaf_value_calculation = None           # If y is one-hot encoded (multi-dim) or not (one-dim)\n",
    "    \n",
    "    def fit(self, X, y, loss=None):\n",
    "        self.one_dim = len(np.shape(y)) == 1\n",
    "        self.root = self._build_tree(X, y)\n",
    "        self.loss = loss\n",
    "    \n",
    "    def _build_tree(self, X, y, current_depth=0):\n",
    "        larget_impurity = 0\n",
    "        best_criteria = None\n",
    "        best_sets = None\n",
    "        \n",
    "        if self.one_dim:\n",
    "            y = np.expand_dims(y, axis=1)\n",
    "        \n",
    "        Xy = np.concatenate((X, y), axis=1)\n",
    "        \n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        if n_samples >= self.min_samples_split and current_depth <= self.max_depth:\n",
    "            for feature_i in range(n_features):\n",
    "                feature_values = np.expand_dims(X[: feature_i], axis=1)\n",
    "                unique_values = np.unique(feature_values)\n",
    "                \n",
    "                for threshold in unique_values:\n",
    "                    Xy1, Xy2 = divide_on_feature(Xy, feature_i, threshold)\n",
    "                    \n",
    "                    if len(Xy1) > 0 and len(Xy2) > 0:\n",
    "                        y1 = Xy1[:, n_features:]\n",
    "                        y2 = Xy2[:, n_features:]\n",
    "                        \n",
    "                        impurity = self._impurity_calculation(y, y1, y2)\n",
    "                        \n",
    "                        if impurity > larget_impurity:\n",
    "                            larget_impurity = impurity\n",
    "                            best_criteria = {\"feature\": feature_i, \"threshold\": threshold}\n",
    "                            \n",
    "                            best_sets = {\n",
    "                                \"leftX\": Xy1[:, :n_features],   # X of left subtree\n",
    "                                \"lefty\": Xy1[:, n_features:],   # y of left subtree\n",
    "                                \"rightX\": Xy2[:, :n_features],  # X of right subtree\n",
    "                                \"righty\": Xy2[:, n_features:]   # y of right subtree\n",
    "                            }\n",
    "                        \n",
    "        if largest_impurity > self.min_impurity:\n",
    "            left_branch = self._build_tree(best_sets.leftX, best_sets.lefty, current_depth + 1)\n",
    "            right_branch = self._build_tree(best_sets.rightX, best_sets.righty, current_depth + 1)\n",
    "            \n",
    "            return DecisionNode(feature_i=best_criteria[\"feature\"], threshold=best_criteria[\"threshold\"], left_branch=left_branch, right_branch=right_branch)\n",
    "        \n",
    "        leaf_value = self._leaf_value_calculation(y)\n",
    "\n",
    "        return DecisionNode(value=leaf_value)\n",
    "    \n",
    "    def predict_value(self, x, tree=None):\n",
    "        if tree is None:\n",
    "            tree = self.root\n",
    "        \n",
    "        if tree.value is not None:\n",
    "            return tree.value\n",
    "        \n",
    "        feature_value = x[tree.feature_i]\n",
    "        \n",
    "        branch = tree.right_branch\n",
    "        if isinstance(feature_value, int) or isinstance(threshold, float):\n",
    "            if feature_value >= tree.threshold:\n",
    "                branch = tree.left_branch\n",
    "        elif feature_value == tree.threshold:\n",
    "            branch = tree.left_branch\n",
    "    \n",
    "        return self.predict_value(x, branch)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_pred = [self.predict_value(sample) for sample in X]\n",
    "        return y_pred \n",
    "    \n",
    "    def print_tree(self, tree=None, indent=\" \"):\n",
    "        if not tree:\n",
    "            tree = self.root\n",
    "        \n",
    "        if tree.value is not None:\n",
    "            print(tree.value)\n",
    "            \n",
    "        else:\n",
    "            print (\"%s:%s? \" % (tree.feature_i, tree.threshold))\n",
    "            # Print the true scenario\n",
    "            print (\"%sT->\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.left_branch, indent + indent)\n",
    "            # Print the false scenario\n",
    "            print (\"%sF->\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.right_branch, indent + indent)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeClassifier(DecisionTree):\n",
    "    def _calculate_information_gain(self, y, y1, y2):\n",
    "        p = len(y1) / len(y)\n",
    "        entropy = calculate_entropy(y)\n",
    "        info_gain = entropy - p * calculate_entropy(y1) - (1 - p) * calculate_entropy(y2)\n",
    "        return info_gain\n",
    "    \n",
    "    def _majority_vote(self, y):\n",
    "        most_common = None\n",
    "        max_count = 0\n",
    "        for label in np.unique(y):\n",
    "            count = len(y[y == label])\n",
    "            if count > max_count:\n",
    "                most_common = label\n",
    "                max_count = count\n",
    "        return most_common\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self._impurity_calculation = self._calculate_information_gain\n",
    "        self._leaf_value_calculation = self._majority_vote\n",
    "        super().fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeRegressor(DecisionTree):\n",
    "    def _calculate_variance_reduction(self, y, y1, y2):\n",
    "        var_tot = calculate_variance(y)\n",
    "        var_1 = calculate_variance(y1)\n",
    "        var_2 = calculate_variance(y2)\n",
    "        frac_1 = len(y1) / len(y)\n",
    "        frac_2 = len(y2) / len(y)\n",
    "        \n",
    "        variance_reduction = var_tot - (frac_1 * var_1 + frac_2 * var_2)\n",
    "        \n",
    "        return sum(variance_reduction)\n",
    "    \n",
    "    def _mean_of_y(self, y):\n",
    "        value = np.mean(y, axis=0)\n",
    "        return value if len(value) > 1 else value[0]\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self._impurity_calculation = self._calculate_variance_reduction\n",
    "        self._leaf_value_calculation = self._mean_of_y\n",
    "        super().fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGBoostRegressionTree(DecisionTree):\n",
    "    def _split(self, y):\n",
    "        col = y.shape[1]//2\n",
    "        y, y_pred = y[:, :col], y[:, col:]\n",
    "        return y, y_pred\n",
    "    \n",
    "    def _gain(self, y, y_pred):\n",
    "        nominator = np.power((y * self.loss.gradient(y, y_pred)).sum(), 2)\n",
    "        denominator = self.loss.hess(y, y_pred).sum()\n",
    "        return 0.5 * (nominator / denominator)\n",
    "    \n",
    "    def _gain_by_taylor(self, y, y1, y2):\n",
    "        y, y_pred = self._split(y)\n",
    "        y1, y1_pred = self._split(y1)\n",
    "        y2, y2_pred = self._split(y2)\n",
    "        \n",
    "        true_gain = self._gain(y1, y1_pred)\n",
    "        false_gain = self._gain(y2, y2_pred)\n",
    "        gain = self._gain(y, y_pred)\n",
    "        return true_gain + false_gain - gain\n",
    "    \n",
    "    def _approximate_update(self, y):\n",
    "        y, y_pred = self._split(y)\n",
    "        gradient = np.sum(y * self.loss.gradient(y, y_pred), axis=0)\n",
    "        hessian = np.sum(self.loss.hess(y, y_pred), axis=0)\n",
    "        update_approximation = gradient / hessian\n",
    "        \n",
    "        return update_approximation\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self._impurity_calculation = self._gain_by_taylor\n",
    "        self._leaf_value_calculation = self._approximate_update\n",
    "        super().fit(X, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:venv-2]",
   "language": "python",
   "name": "conda-env-venv-2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
